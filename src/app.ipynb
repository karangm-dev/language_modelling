{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(input_directory):\n",
    "    file_contents = \"\"\n",
    "    for file_name in glob.glob(input_directory + \"/*\"):\n",
    "        print(\"Prepocessing: {}\".format(file_name))\n",
    "        file_pointer = open(file_name, \"r\")\n",
    "\n",
    "        # Read file contents\n",
    "        file_content = file_pointer.read()\n",
    "\n",
    "        # Remove duplicate spaces\n",
    "        file_content = re.sub(' +', ' ', file_content)\n",
    "\n",
    "        # Remove new line characters\n",
    "        file_content = file_content.replace(\"\\n\", \" \")\n",
    "\n",
    "        if file_contents == \"\":\n",
    "            file_contents = file_content\n",
    "        else:\n",
    "            file_contents = file_contents + ' ' + file_content\n",
    "        file_pointer.close()\n",
    "    return file_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_tokens(data):\n",
    "    word_tokens = word_tokenize(data)\n",
    "    word_tokens = [word_token.lower() for word_token in word_tokens]\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_low_count_words(word_tokens, cut_off_count):\n",
    "    word_tokens_with_count = Counter(word_tokens)\n",
    "    candidate_words = {}\n",
    "    for word in word_tokens_with_count:\n",
    "        if word_tokens_with_count[word] <= cut_off_count:\n",
    "            candidate_words[word] = 1\n",
    "    \n",
    "    ## Remove all the words <= cut_off_count\n",
    "    for i in range(len(word_tokens)):\n",
    "        if word_tokens[i] in candidate_words:\n",
    "            word_tokens[i] = 'UNK'\n",
    "\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_ngrams(word_tokens, n):\n",
    "    ngrams_zip = zip(*[word_tokens[i:] for i in range(n)])\n",
    "    ngrams_list = [\" \".join(element) for element in ngrams_zip]\n",
    "    ngrams_keys_counts = Counter(ngrams_list)\n",
    "    return ngrams_keys_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nltk_generate_ngrams(word_tokens, n):\n",
    "    ngrams_result = list(ngrams(word_tokens, n))\n",
    "    ngrams_keys_counts = Counter(ngrams_result)\n",
    "    return ngrams_keys_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1\n",
    "Across all files in the directory (counted together), report the unigram, bigram, and trigram wordlevel counts. Submit these counts in a file named ngramCounts.txt.\n",
    "\n",
    "Note: You can use any word tokenizer to tokenize the dataset e.g. nltk word tokenize, although\n",
    "for creating the n-grams do not use any libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_1():\n",
    "    print(\"##### Loading train date\")\n",
    "    train_data = load_data('../input/train')\n",
    "    #     train_data = load_data('../input/custom_train')\n",
    "\n",
    "    print(\"##### Tokenize train data\")\n",
    "    train_word_tokens = get_word_tokens(train_data)\n",
    "    print(\"Number of original word tokens: {}\".format(len(train_word_tokens)))\n",
    "    \n",
    "    print(\"### Replace word tokens <= 3 with 'UNK\")\n",
    "    train_word_tokens = replace_low_count_words(train_word_tokens, 3)\n",
    "    print(\"Number of tokens after replacement: {}\".format(len(train_word_tokens)))\n",
    "\n",
    "    print(\"##### Get Unigrams\")\n",
    "    unigrams = generate_ngrams(train_word_tokens, 1)\n",
    "    print(\"Unigrams counts: {}, nltk count: {}\".format(len(unigrams), len( nltk_generate_ngrams(train_word_tokens, 1))))\n",
    "\n",
    "    print(\"##### Get Bigrams\")\n",
    "    bigrams = generate_ngrams(train_word_tokens, 2)\n",
    "    print(\"Bigrams counts: {}, nltk count: {}\".format(len(bigrams), len( nltk_generate_ngrams(train_word_tokens, 2))))\n",
    "\n",
    "    print(\"##### Get Trigrams\")\n",
    "    trigrams = generate_ngrams(train_word_tokens, 3)\n",
    "    print(\"Trigrams counts: {}, nltk count: {}\".format(len(trigrams), len(nltk_generate_ngrams(train_word_tokens, 3))))\n",
    "    \n",
    "    # Write to output file ngramCounts.txt\n",
    "    print(\"Writing to output file ngramCounts.txt\")\n",
    "    \n",
    "    with open('../output/ngramCounts.txt', 'w') as output_file:\n",
    "        output_file.write('Unigrams: {}\\n'.format(len(unigrams)))\n",
    "        output_file.write('Bigrams: {}\\n'.format(len(bigrams)))\n",
    "        output_file.write('Trigrams: {}\\n'.format(len(trigrams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Loading train date\n",
      "Prepocessing: ../input/train/a894e49e-a0a6-4851-be23-4da89a52bb8e.txt\n",
      "Prepocessing: ../input/train/14d44997-7510-4777-adf0-5e4dd387e0bf.txt\n",
      "Prepocessing: ../input/train/521ce35b-288c-4b12-a7a8-70b836290f90.txt\n",
      "Prepocessing: ../input/train/9ae9dae7-69a0-4116-9622-448f154bc269.txt\n",
      "Prepocessing: ../input/train/0493e223-a0e2-4c6f-ade9-7172f35c18b1.txt\n",
      "Prepocessing: ../input/train/da059d4f-19ac-4130-858a-f6241b56fe39.txt\n",
      "Prepocessing: ../input/train/30381986-3d6b-4227-9733-9483ead7343d.txt\n",
      "Prepocessing: ../input/train/e0a13927-26e0-4ca8-b1a0-864b7604e566.txt\n",
      "Prepocessing: ../input/train/5d384641-37e5-4b1c-b4d6-0ee935141ecb.txt\n",
      "Prepocessing: ../input/train/904393ad-fbc1-4512-8705-ce1c005c4915.txt\n",
      "##### Tokenize train data\n",
      "Number of original word tokens: 2495360\n",
      "### Replace word tokens <= 3 with 'UNK\n",
      "Number of tokens after replacement: 2495360\n",
      "##### Get Unigrams\n",
      "Unigrams counts: 12936, nltk count: 12936\n",
      "##### Get Bigrams\n",
      "Bigrams counts: 386929, nltk count: 386929\n",
      "##### Get Trigrams\n",
      "Trigrams counts: 1219111, nltk count: 1219111\n",
      "Writing to output file ngramCounts.txt\n"
     ]
    }
   ],
   "source": [
    "run_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2\n",
    "\n",
    "For the given test dataset: https://www.dropbox.com/s/ik98szmqbsq2wtd/test.zip?dl=0\n",
    "Calculate the perplexity for each file in the test set using linear interpolation smoothing method.\n",
    "For determining the λs for linear interpolation, you can divide the training data into a new training\n",
    "set (80%) and a held-out set (20%) , then using grid search method.\n",
    "\n",
    "1. First, report all the candidate lambdas used for grid search and the corresponding perplexities you got on the held-out set\n",
    "\n",
    "2. Report the best λs chosen from the grid search, and explain why it’s chosen (i.e. leveraging the perplexities achieved on the held-out set).\n",
    "\n",
    "3. Report the perplexity for each file in the test set (use the best λs obtained from grid search to calculate perplexity on test set).\n",
    "\n",
    "4. Based on the test file’s perplexities you got write a brief observation comparing the test files. Submit these perplexities and your report in a file named perplexitiesInterpolation.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_candidate_lamdas(lambda_pool):\n",
    "    candidate_lambdas = []\n",
    "    for i in lambda_pool:\n",
    "        for j in lambda_pool:\n",
    "            if i+j < 1:\n",
    "                candidate_lambdas.append([i, j, round(1-(i+j), 1)])\n",
    "    return candidate_lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_eval_data_with_UNK(eval_word_tokens, train_unigrams):\n",
    "    for i in range(len(eval_word_tokens)):\n",
    "        if eval_word_tokens[i] not in train_unigrams:\n",
    "            eval_word_tokens[i] = 'UNK' \n",
    "    return eval_word_tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unigram_probability(eval_word, corpus_length, train_unigrams):\n",
    "    numerator = eval_word.split(\" \")[-1:][0]\n",
    "    probability =  train_unigrams[numerator]/corpus_length\n",
    "    return(probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bigram_probabiltiy(eval_word, train_unigrams, train_bigrams):\n",
    "    denominator_word_tokens = eval_word.split(\" \")[-2:-1]\n",
    "    denominator_word = \" \".join(denominator_word_tokens)\n",
    "    numerator_word = eval_word.split(\" \")[-2:]\n",
    "    probability =  (train_bigrams[eval_word]) /(train_unigrams[denominator_word])\n",
    "    return(probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_trigram_probabiltiy(eval_word, train_bigrams, train_trigrams):\n",
    "    denominator_word_tokens = eval_word.split(\" \")[-3:-1]\n",
    "    denominator_word = \" \".join(denominator_word_tokens)\n",
    "    if denominator_word in train_bigrams:\n",
    "        probability =  (train_trigrams[eval_word])/(train_bigrams[denominator_word])\n",
    "        return probability\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_interpolation(lambdas, train_unigrams, train_bigrams, train_trigrams, eval_trigrams):\n",
    "    # Calculate corpous length\n",
    "    corpus_length = 0\n",
    "    for unigram in train_unigrams:\n",
    "        corpus_length = corpus_length + train_unigrams[unigram]\n",
    "    \n",
    "    result = 0\n",
    "    # Calculate the linear interpolation\n",
    "    [lambda_1, lambda_2, lambda_3] = lambdas\n",
    "    for eval_trigram in eval_trigrams:\n",
    "        unigram_probability = get_unigram_probability(eval_trigram, corpus_length, train_unigrams)\n",
    "        bigram_probabiltiy = get_bigram_probabiltiy(eval_trigram, train_unigrams, train_bigrams)\n",
    "        trigram_probabiltiy = get_trigram_probabiltiy(eval_trigram, train_bigrams, train_trigrams)\n",
    "        probability = (lambda_1 * trigram_probabiltiy) + (lambda_2 * bigram_probabiltiy) + (lambda_3 * unigram_probability)\n",
    "        log_probability = math.log(probability)\n",
    "        result = result + log_probability    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_perplexity(log_probability_score, corpus_length):\n",
    "    perplexity = (math.e ** (-1/corpus_length * log_probability_score))\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_lambdas(candidate_lambdas, train_unigrams, train_bigrams, train_trigrams, eval_trigrams, eval_corpus_length):\n",
    "    output_file = open('../output/perplexitiesInterpolation.txt', 'w')\n",
    "    best_parameters = None\n",
    "    best_score = None\n",
    "    for i in range(len(candidate_lambdas)):\n",
    "        result = linear_interpolation(candidate_lambdas[i], train_unigrams, train_bigrams, train_trigrams, eval_trigrams)\n",
    "        if best_score == None:\n",
    "            best_score = result\n",
    "            best_parameters = candidate_lambdas[i]\n",
    "        else:\n",
    "            if result > best_score:\n",
    "                best_score = result\n",
    "                best_parameters = candidate_lambdas[i]\n",
    "        # Calculate Perplexity\n",
    "        perplexity = calculate_perplexity(result, eval_corpus_length)\n",
    "        # Write to output file        \n",
    "        output_file.write('Lambdas: {}, Perplexity Score: {} \\n'.format(candidate_lambdas[i], perplexity)) \n",
    "    output_file.close()\n",
    "    return best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_linear_interpolation_perplexity_for_test_data(input_directory, lambdas, train_unigrams, train_bigrams, train_trigrams):\n",
    "    perplexities_interpolation_output_file = open('../output/perplexitiesInterpolation.txt', 'a')\n",
    "    perplexities_interpolation_output_file.write('\\n')\n",
    "    \n",
    "    for file_name in glob.glob(input_directory + \"/*\"):\n",
    "        print(\"Prepocessing for test file : {}\".format(file_name))\n",
    "        file_pointer = open(file_name, \"r\")\n",
    "\n",
    "        # Read file contents\n",
    "        file_content = file_pointer.read()\n",
    "\n",
    "        # Remove duplicate spaces\n",
    "        file_content = re.sub(' +', ' ', file_content)\n",
    "\n",
    "        # Remove new line characters\n",
    "        file_content = file_content.replace(\"\\n\", \" \")\n",
    "\n",
    "        print(\"##### Tokenize test data\")\n",
    "        test_word_tokens = get_word_tokens(file_content)\n",
    "        print(\"Number of original test word tokens: {}\".format(len(test_word_tokens)))\n",
    "\n",
    "        print(\"Replacing words in test data with UNK that is not present in train data\")\n",
    "        test_word_tokens = replace_eval_data_with_UNK(test_word_tokens, train_unigrams)\n",
    "\n",
    "        print(\"Trigram for test data\")\n",
    "        test_trigrams = generate_ngrams(test_word_tokens, 3)\n",
    "        test_corpus_length = len(test_word_tokens)\n",
    "        \n",
    "        log_probability = linear_interpolation(lambdas, train_unigrams, train_bigrams, train_trigrams, test_trigrams)\n",
    "        perplexity = calculate_perplexity(log_probability, test_corpus_length)\n",
    "        perplexities_interpolation_output_file.write('Filename: {}, Perplexity: {}\\n'.format(file_name, perplexity))\n",
    "        \n",
    "    perplexities_interpolation_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_2():\n",
    "    print(\"##### Loading input data\")\n",
    "    input_data = load_data('../input/train')\n",
    "#     input_data = load_data('../input/custom_train')\n",
    "    \n",
    "    print(\"##### Tokenize input data\")\n",
    "    input_word_tokens = get_word_tokens(input_data)\n",
    "    print(\"Number of original word tokens: {}\".format(len(input_word_tokens)))\n",
    "    \n",
    "    eighty_percentile = round(len(input_word_tokens) * 0.8)\n",
    "    train_word_tokens = input_word_tokens[:eighty_percentile]\n",
    "    dev_word_tokens = input_word_tokens[eighty_percentile:]\n",
    "    print(\"Trian: {}, Dev: {}, Total: {}\".format(len(train_word_tokens), len(dev_word_tokens), len(input_word_tokens)))\n",
    "    print(\"Percentage: Trian: {}, Dev: {}\".format(len(train_word_tokens)/len(input_word_tokens), len(dev_word_tokens)/len(input_word_tokens)))\n",
    "    \n",
    "    print(\"### Replace train word tokens <= 3 with 'UNK\")\n",
    "    train_word_tokens = replace_low_count_words(train_word_tokens, 3)\n",
    "    print(\"Number of train tokens after replacement: {}\".format(len(train_word_tokens)))\n",
    "    \n",
    "    train_unigrams = generate_ngrams(train_word_tokens, 1)\n",
    "    train_bigrams = generate_ngrams(train_word_tokens, 2)\n",
    "    train_trigrams = generate_ngrams(train_word_tokens, 3)\n",
    "    print(\"Training data Counts of Unigrams: {}, Bigrams: {}, Trigrams: {}\".format(len(train_unigrams), len(train_bigrams), len(train_trigrams)))\n",
    "    \n",
    "    lambda_pool = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    candidate_lambdas = get_candidate_lamdas(lambda_pool)\n",
    "    \n",
    "    print(\"Replacing words in eval data with UNK that is not present in train data\")\n",
    "    dev_word_tokens = replace_eval_data_with_UNK(dev_word_tokens, train_unigrams)\n",
    "    dev_corpus_length = len(dev_word_tokens)\n",
    "    print(\"Trigram for held out data\")\n",
    "    dev_trigrams = generate_ngrams(dev_word_tokens, 3)\n",
    "\n",
    "    lambdas = get_best_lambdas(candidate_lambdas, train_unigrams, train_bigrams, train_trigrams, dev_trigrams, dev_corpus_length)\n",
    "    print(\"Best Lambdas: {}\".format(lambdas))\n",
    "    with open('../output/perplexitiesInterpolation.txt', 'a') as perplexities_interpolation_output_file:\n",
    "        perplexities_interpolation_output_file.write('\\n Choosen lambdas: {}\\n'.format(lambdas))\n",
    "        \n",
    "#     calculate_perplexity_for_test_data('../input/custom_test', lambdas, train_unigrams, train_bigrams, train_trigrams)\n",
    "    calculate_linear_interpolation_perplexity_for_test_data('../input/test', lambdas, train_unigrams, train_bigrams, train_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Loading input data\n",
      "Prepocessing: ../input/train/a894e49e-a0a6-4851-be23-4da89a52bb8e.txt\n",
      "Prepocessing: ../input/train/14d44997-7510-4777-adf0-5e4dd387e0bf.txt\n",
      "Prepocessing: ../input/train/521ce35b-288c-4b12-a7a8-70b836290f90.txt\n",
      "Prepocessing: ../input/train/9ae9dae7-69a0-4116-9622-448f154bc269.txt\n",
      "Prepocessing: ../input/train/0493e223-a0e2-4c6f-ade9-7172f35c18b1.txt\n",
      "Prepocessing: ../input/train/da059d4f-19ac-4130-858a-f6241b56fe39.txt\n",
      "Prepocessing: ../input/train/30381986-3d6b-4227-9733-9483ead7343d.txt\n",
      "Prepocessing: ../input/train/e0a13927-26e0-4ca8-b1a0-864b7604e566.txt\n",
      "Prepocessing: ../input/train/5d384641-37e5-4b1c-b4d6-0ee935141ecb.txt\n",
      "Prepocessing: ../input/train/904393ad-fbc1-4512-8705-ce1c005c4915.txt\n",
      "##### Tokenize input data\n",
      "Number of original word tokens: 2495360\n",
      "Trian: 1996288, Dev: 499072, Total: 2495360\n",
      "Percentage: Trian: 0.8, Dev: 0.2\n",
      "### Replace train word tokens <= 3 with 'UNK\n",
      "Number of train tokens after replacement: 1996288\n",
      "Training data Counts of Unigrams: 11666, Bigrams: 331252, Trigrams: 1011594\n",
      "Replacing words in eval data with UNK that is not present in train data\n",
      "Trigram for held out data\n",
      "Best Lambdas: [0.3, 0.1, 0.6]\n",
      "Prepocessing for test file : ../input/test/test01.txt\n",
      "##### Tokenize test data\n",
      "Number of original test word tokens: 71546\n",
      "Replacing words in test data with UNK that is not present in train data\n",
      "Trigram for test data\n",
      "Prepocessing for test file : ../input/test/test02.txt\n",
      "##### Tokenize test data\n",
      "Number of original test word tokens: 248968\n",
      "Replacing words in test data with UNK that is not present in train data\n",
      "Trigram for test data\n"
     ]
    }
   ],
   "source": [
    "run_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3\n",
    "Build another language model with add-λ smoothing. Use λ = 0.1 and λ = 0.3.\n",
    "1. Report the perplexity for each file in the test set (for both the λ values).\n",
    "2. Based on the test file’s perplexities you got write a brief observation comparing the test files.\n",
    "Submit these perplexities and your report in a file named perplexitiesAddLambda.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_k(k, vocab_size, train_bigrams, train_trigrams, eval_trigrams):\n",
    "    result = 0\n",
    "    for eval_trigram in eval_trigrams:\n",
    "        denominator_word_tokens = eval_trigram.split(\" \")[-3:-1]\n",
    "        denominator_word = \" \".join(denominator_word_tokens)\n",
    "        probability = (train_trigrams[eval_trigram] + k)/ (train_bigrams[denominator_word] + vocab_size)\n",
    "        log_probability = math.log(probability)\n",
    "        result = result + log_probability    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_add_k_perplexity_for_test_data(input_directory, k_list, vocab_size, train_unigrams, train_bigrams, train_trigrams):\n",
    "    perplexities_add_k_output_file = open('../output/perplexitiesAddLambda.txt', 'w')\n",
    "    for k in k_list:\n",
    "        perplexities_add_k_output_file.write('Lambda: {}\\n'.format(k))\n",
    "        for file_name in glob.glob(input_directory + \"/*\"):\n",
    "            print(\"Prepocessing for test file : {}\".format(file_name))\n",
    "            file_pointer = open(file_name, \"r\")\n",
    "\n",
    "            # Read file contents\n",
    "            file_content = file_pointer.read()\n",
    "\n",
    "            # Remove duplicate spaces\n",
    "            file_content = re.sub(' +', ' ', file_content)\n",
    "\n",
    "            # Remove new line characters\n",
    "            file_content = file_content.replace(\"\\n\", \" \")\n",
    "\n",
    "            print(\"##### Tokenize test data\")\n",
    "            test_word_tokens = get_word_tokens(file_content)\n",
    "            print(\"Number of original test word tokens: {}\".format(len(test_word_tokens)))\n",
    "\n",
    "            print(\"Replacing words in test data with UNK that is not present in train data\")\n",
    "            test_word_tokens = replace_eval_data_with_UNK(test_word_tokens, train_unigrams)\n",
    "\n",
    "            print(\"Trigram for test data\")\n",
    "            test_trigrams = generate_ngrams(test_word_tokens, 3)\n",
    "            test_corpus_length = len(test_word_tokens)\n",
    "\n",
    "            log_probability = add_k(k, vocab_size, train_bigrams, train_trigrams, test_trigrams)\n",
    "            perplexity = calculate_perplexity(log_probability, test_corpus_length)\n",
    "            perplexities_add_k_output_file.write('Filename: {}, Perplexity: {}\\n'.format(file_name, perplexity))\n",
    "\n",
    "    perplexities_add_k_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_3():\n",
    "    print(\"##### Loading input data\")\n",
    "    input_data = load_data('../input/train')\n",
    "#     input_data = load_data('../input/custom_train')\n",
    "    \n",
    "    print(\"##### Tokenize input data\")\n",
    "    input_word_tokens = get_word_tokens(input_data)\n",
    "    print(\"Number of original word tokens: {}\".format(len(input_word_tokens)))\n",
    "    \n",
    "    eighty_percentile = round(len(input_word_tokens) * 0.8)\n",
    "    train_word_tokens = input_word_tokens[:eighty_percentile]\n",
    "    dev_word_tokens = input_word_tokens[eighty_percentile:]\n",
    "    print(\"Trian: {}, Dev: {}, Total: {}\".format(len(train_word_tokens), len(dev_word_tokens), len(input_word_tokens)))\n",
    "    print(\"Percentage: Trian: {}, Dev: {}\".format(len(train_word_tokens)/len(input_word_tokens), len(dev_word_tokens)/len(input_word_tokens)))\n",
    "    \n",
    "    print(\"### Replace train word tokens <= 3 with 'UNK\")\n",
    "    train_word_tokens = replace_low_count_words(train_word_tokens, 3)\n",
    "    print(\"Number of train tokens after replacement: {}\".format(len(train_word_tokens)))\n",
    "    \n",
    "    train_unigrams = generate_ngrams(train_word_tokens, 1)\n",
    "    vocab_size = len(train_unigrams.keys())\n",
    "    train_bigrams = generate_ngrams(train_word_tokens, 2)\n",
    "    train_trigrams = generate_ngrams(train_word_tokens, 3)\n",
    "    print(\"Training data Counts of Unigrams: {}, Bigrams: {}, Trigrams: {}\".format(len(train_unigrams), len(train_bigrams), len(train_trigrams)))\n",
    "    \n",
    "#     calculate_perplexity_for_test_data('../input/custom_test', lambdas, train_unigrams, train_bigrams, train_trigrams)\n",
    "    calculate_add_k_perplexity_for_test_data('../input/test', [0.1, 0.3], vocab_size, train_unigrams, train_bigrams, train_trigrams)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Loading input data\n",
      "Prepocessing: ../input/train/a894e49e-a0a6-4851-be23-4da89a52bb8e.txt\n",
      "Prepocessing: ../input/train/14d44997-7510-4777-adf0-5e4dd387e0bf.txt\n",
      "Prepocessing: ../input/train/521ce35b-288c-4b12-a7a8-70b836290f90.txt\n",
      "Prepocessing: ../input/train/9ae9dae7-69a0-4116-9622-448f154bc269.txt\n",
      "Prepocessing: ../input/train/0493e223-a0e2-4c6f-ade9-7172f35c18b1.txt\n",
      "Prepocessing: ../input/train/da059d4f-19ac-4130-858a-f6241b56fe39.txt\n",
      "Prepocessing: ../input/train/30381986-3d6b-4227-9733-9483ead7343d.txt\n",
      "Prepocessing: ../input/train/e0a13927-26e0-4ca8-b1a0-864b7604e566.txt\n",
      "Prepocessing: ../input/train/5d384641-37e5-4b1c-b4d6-0ee935141ecb.txt\n",
      "Prepocessing: ../input/train/904393ad-fbc1-4512-8705-ce1c005c4915.txt\n",
      "##### Tokenize input data\n",
      "Number of original word tokens: 2495360\n",
      "Trian: 1996288, Dev: 499072, Total: 2495360\n",
      "Percentage: Trian: 0.8, Dev: 0.2\n",
      "### Replace train word tokens <= 3 with 'UNK\n",
      "Number of train tokens after replacement: 1996288\n",
      "Training data Counts of Unigrams: 11666, Bigrams: 331252, Trigrams: 1011594\n",
      "Prepocessing for test file : ../input/test/test01.txt\n",
      "##### Tokenize test data\n",
      "Number of original test word tokens: 71546\n",
      "Replacing words in test data with UNK that is not present in train data\n",
      "Trigram for test data\n",
      "Prepocessing for test file : ../input/test/test02.txt\n",
      "##### Tokenize test data\n",
      "Number of original test word tokens: 248968\n",
      "Replacing words in test data with UNK that is not present in train data\n",
      "Trigram for test data\n",
      "Prepocessing for test file : ../input/test/test01.txt\n",
      "##### Tokenize test data\n",
      "Number of original test word tokens: 71546\n",
      "Replacing words in test data with UNK that is not present in train data\n",
      "Trigram for test data\n",
      "Prepocessing for test file : ../input/test/test02.txt\n",
      "##### Tokenize test data\n",
      "Number of original test word tokens: 248968\n",
      "Replacing words in test data with UNK that is not present in train data\n",
      "Trigram for test data\n"
     ]
    }
   ],
   "source": [
    "run_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4\n",
    "Based on your observation from above questions, compare linear interpolation and add-lambda\n",
    "smoothing by listing out their pros and cons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity is lower in linear interpolation compared to add-lambda indicating linear interpolation trigram model is better.\n",
    "\n",
    "However, an extra computation is required to identify the best parameters for linear interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "language_modelling",
   "language": "python",
   "name": "language_modelling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
